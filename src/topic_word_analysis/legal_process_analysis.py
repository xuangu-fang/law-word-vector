
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Topic Analysis - æ³•å¾‹æµç¨‹åˆ†æå™¨

åŠŸèƒ½ï¼š
1. åˆ†æ"ç«‹æ³•ã€å¸æ³•ã€æ‰§æ³•ã€å®ˆæ³•"å››ä¸ªæ³•å¾‹æµç¨‹ç»´åº¦  
2. è®¡ç®—"æ³•æ²»"/"æ³•åˆ¶"ä¸å„ç»´åº¦çš„ç›¸ä¼¼åº¦
3. æ”¯æŒå¤šç§era-keywordç»„åˆå’Œå½’ä¸€åŒ–æ¨¡å¼
4. ç”Ÿæˆé›·è¾¾å›¾ã€è¶‹åŠ¿å›¾ã€çƒ­åŠ›å›¾
5. ä½¿ç”¨General Unionæ¨¡å¼ç¡®ä¿è¯åŒ…ä¸€è‡´æ€§

è¾“å‡ºç›®å½•ï¼šoutput/topic_analysis/legal_process/
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from gensim.models import KeyedVectors
import json
from itertools import chain
import warnings
warnings.filterwarnings('ignore')

# Setup plotting style and fonts
plt.style.use('seaborn-v0_8-whitegrid')
sns.set(font_scale=1.2)
sns.set_style("whitegrid")
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®matplotlibä¸æ˜¾ç¤ºå›¾å½¢ï¼Œåªä¿å­˜
import matplotlib
matplotlib.use('Agg')

PROJECT_ROOT = Path(__file__).parent.parent.parent
MODELS_DIR = PROJECT_ROOT / "models" / "fine_tuned_vectors_flexible"
OUTPUT_DIR = PROJECT_ROOT / "output" / "topic_analysis" / "legal_process"
DATA_PATH = PROJECT_ROOT / "output" / "topic_analysis" / "legal_process" / "topic_word_sets_legal_process.json"

class LegalProcessAnalyzer:
    def __init__(self, models):
        if not models:
            raise ValueError("No models provided.")
        self.models = models
        self.output_dir = OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        with open(DATA_PATH, 'r', encoding='utf-8') as f:
            self.topic_word_sets = json.load(f)
        
        # åˆ›å»ºgeneral union wordset (è·¨å…³é”®è¯+è·¨æ—¶æœŸ)
        self.general_union_wordset = self._create_general_union_wordset()

    def _get_word_set(self, keyword, era, use_union=False, use_general_union=False):
        """Helper to retrieve word sets for a given keyword and era."""
        if use_general_union:
            # è·¨å…³é”®è¯+è·¨æ—¶æœŸçš„å®Œå…¨å¹¶é›†
            return self.general_union_wordset
        elif use_union:
            # ä»…è·¨æ—¶æœŸçš„å¹¶é›†ï¼ˆé’ˆå¯¹ç‰¹å®šå…³é”®è¯ï¼‰
            all_words = {}
            for era_data in self.topic_word_sets.get(keyword, {}).values():
                for topic, words in era_data.items():
                    if topic not in all_words:
                        all_words[topic] = set()
                    all_words[topic].update(words)
            return {topic: list(words) for topic, words in all_words.items()}
        
        return self.topic_word_sets.get(keyword, {}).get(era, {})

    def _create_general_union_wordset(self):
        """åˆ›å»ºè·¨å…³é”®è¯+è·¨æ—¶æœŸçš„å®Œå…¨å¹¶é›†è¯åŒ…"""
        try:
            # ç›´æ¥è¯»å–ç°æœ‰çš„JSONæ–‡ä»¶
            general_union_path = self.output_dir / "general_union_wordset_legal_process.json"
            with open(general_union_path, 'r', encoding='utf-8') as f:
                result = json.load(f)
            
            print(f"General Union Wordset ç»Ÿè®¡ (ä»æ–‡ä»¶è¯»å–):")
            for topic, words in result.items():
                print(f"  {topic}: {len(words)} ä¸ªè¯")
            
            return result
        except FileNotFoundError:
            print("è­¦å‘Š: æœªæ‰¾åˆ°general_union_wordset_legal_process.jsonæ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°çš„")
        general_union = {}
        
        # éå†æ‰€æœ‰å…³é”®è¯ï¼ˆæ³•æ²»ã€æ³•åˆ¶ç­‰ï¼‰
        for keyword, keyword_data in self.topic_word_sets.items():
            # éå†æ‰€æœ‰æ—¶æœŸ
            for era, era_data in keyword_data.items():
                # éå†æ‰€æœ‰topic
                for topic, words in era_data.items():
                    if topic not in general_union:
                        general_union[topic] = set()
                    general_union[topic].update(words)
        
        # è½¬æ¢ä¸ºlistå¹¶æ’åº
        result = {topic: sorted(list(word_set)) for topic, word_set in general_union.items()}
        
        print(f"General Union Wordset ç»Ÿè®¡:")
        for topic, words in result.items():
            print(f"  {topic}: {len(words)} ä¸ªè¯")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        general_union_path = self.output_dir / "general_union_wordset_legal_process.json"
        with open(general_union_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"General Union Wordset å·²ä¿å­˜åˆ°: {general_union_path}")
        
        return result

    def calculate_similarities(self, era_keyword_map, use_union=False, use_general_union=False, normalize=None):
        """
        è®¡ç®—ç‰¹å®šæ—¶æœŸ-ç‰¹å®šç»´åº¦çš„ç›¸ä¼¼åº¦
        
        å¯¹äºæ¯ä¸ªera-topicç»„åˆï¼š
        - ä½¿ç”¨è¯¥eraçš„è¯å‘é‡æ¨¡å‹
        - ä½¿ç”¨æŒ‡å®šçš„å…³é”®è¯ï¼ˆæ³•æ²»/æ³•åˆ¶ç­‰ï¼‰
        - ä¸è¯¥eraè¯¥topicçš„è¯åŒ…è®¡ç®—ç›¸ä¼¼åº¦
        
        Args:
            era_keyword_map (dict): Maps eras to keywords (e.g., {'era1': 'æ³•åˆ¶', 'era2': 'æ³•æ²»'}).
                                  Supports mixed mode: {'era2': ['æ³•åˆ¶', 'æ³•æ²»']}.
            use_union (bool): If True, use the union of word sets across all eras.
            use_general_union (bool): If True, use complete union across keywords and eras.
            normalize (str): Normalization method ('cross_era', 'same_era', None).

        Returns:
            pd.DataFrame: A DataFrame with similarity scores.
        """
        similarity_data = []
        eras = sorted(era_keyword_map.keys())
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„topicsï¼ˆåº”è¯¥æ˜¯ï¼šå‘å±•ã€ç§©åºã€è§„èŒƒã€æƒåŠ›é™åˆ¶ï¼‰
        all_topics = set()
        for keyword_data in self.topic_word_sets.values():
            for era_data in keyword_data.values():
                all_topics.update(era_data.keys())
        all_topics = sorted(list(all_topics))
        
        print(f"å‘ç°çš„topics: {all_topics}")

        for era in eras:
            if era not in self.models:
                print(f"è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°era {era}çš„æ¨¡å‹")
                continue
                
            model = self.models[era]
            keywords = era_keyword_map[era]
            if isinstance(keywords, str):
                keywords = [keywords]

            era_similarities = {"era": era}
            
            for topic in all_topics:
                topic_similarities = []
                
                for keyword in keywords:
                    # è·å–è¯¥eraè¯¥keywordçš„topicè¯åŒ…
                    word_set = self._get_word_set(keyword, era, use_union=use_union, use_general_union=use_general_union)
                    
                    topic_words = word_set.get(topic, [])
                    
                    if not topic_words:
                        print(f"è­¦å‘Š: {era}-{keyword}-{topic} æ²¡æœ‰æ‰¾åˆ°è¯åŒ…")
                        continue
                    
                    # è®¡ç®—è¯¥å…³é”®è¯ä¸è¯¥topicè¯åŒ…çš„ç›¸ä¼¼åº¦
                    if keyword not in model:
                        print(f"è­¦å‘Š: å…³é”®è¯ '{keyword}' ä¸åœ¨ {era} æ¨¡å‹ä¸­")
                        continue
                    
                    valid_sims = []
                    for word in topic_words:
                        if word in model and word != keyword:
                            try:
                                sim = model.similarity(keyword, word)
                                valid_sims.append(sim)
                            except KeyError:
                                pass
                    
                    if valid_sims:
                        avg_sim = np.mean(valid_sims)
                        topic_similarities.append(avg_sim)
                        print(f"{era}-{keyword}-{topic}: {len(valid_sims)}ä¸ªæœ‰æ•ˆè¯, å¹³å‡ç›¸ä¼¼åº¦={avg_sim:.3f}")

                # å¦‚æœæ˜¯æ··åˆæ¨¡å¼ï¼ˆå¤šä¸ªå…³é”®è¯ï¼‰ï¼Œå–å¹³å‡
                if topic_similarities:
                    era_similarities[topic] = np.mean(topic_similarities)
                else:
                    era_similarities[topic] = 0.0
            
            similarity_data.append(era_similarities)

        df = pd.DataFrame(similarity_data)
        
        if normalize and not df.empty:
            if normalize == 'same_era':
                # åŒä¸€eraå†…çš„å„topicç›¸ä¼¼åº¦å½’ä¸€åŒ–ï¼ˆå’Œä¸º1ï¼‰
                df.iloc[:, 1:] = df.iloc[:, 1:].div(df.iloc[:, 1:].sum(axis=1), axis=0).fillna(0)
            elif normalize == 'cross_era':
                # è·¨eraæ ‡å‡†åŒ–
                for col in df.columns[1:]:
                    col_data = df[col]
                    if col_data.max() > col_data.min():
                        df[col] = (col_data - col_data.min()) / (col_data.max() - col_data.min())
                    else:
                        df[col] = 0
        
        return df

    def _create_output_path(self, settings):
        """Creates a descriptive output path based on analysis settings."""
        path_parts = []
        for key, value in settings.items():
            if isinstance(value, bool) and value:
                path_parts.append(key)
            elif isinstance(value, str) and value is not None:
                path_parts.append(f"{key}_{value}")
            elif isinstance(value, list):
                str_value = '_'.join(map(str, value))
                path_parts.append(f"{key}_{str_value}")
        
        setting_str = "-".join(filter(None, path_parts))
        path = self.output_dir / setting_str
        path.mkdir(parents=True, exist_ok=True)
        return path

    def plot_radar(self, df, path, title):
        """Generates and saves a radar plot."""
        labels = df.columns[1:]
        num_vars = len(labels)
        
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

        for i, row in df.iterrows():
            data = row.drop('era').tolist()
            data += data[:1]
            ax.plot(angles, data, label=row['era'])
            ax.fill(angles, data, alpha=0.25)

        ax.set_yticklabels([])
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        plt.title(title)
        plt.savefig(path / "radar_chart.png", dpi=300)
        plt.close()

    def plot_trend(self, df, path, title):
        """Generates and saves a trend plot."""
        plt.figure(figsize=(12, 7))
        for column in df.columns[1:]:
            sns.lineplot(data=df, x='era', y=column, marker='o', label=column)
        plt.title(title)
        plt.ylabel("Similarity")
        plt.xlabel("Era")
        plt.legend(title="Topic")
        plt.tight_layout()
        plt.savefig(path / "trend_chart.png", dpi=300)
        plt.close()

    def plot_heatmap(self, df, path, title):
        """Generates and saves a heatmap with era on x-axis."""
        # è½¬ç½®æ•°æ®ï¼Œä½¿eraåœ¨xè½´ï¼Œtopicsåœ¨yè½´
        df_transposed = df.set_index('era').T
        plt.figure(figsize=(10, 8))
        # å¦‚æœéœ€è¦è‡ªå®šä¹‰xè½´å’Œyè½´çš„ticksåç§°ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®xticklabelså’Œyticklabelså‚æ•°
        # ä¾‹å¦‚ï¼Œå‡è®¾ä½ æƒ³è‡ªå®šä¹‰xè½´ä¸º["æ—¶æœŸä¸€", "æ—¶æœŸäºŒ", "æ—¶æœŸä¸‰"]ï¼Œyè½´ä¸º["ç»´åº¦A", "ç»´åº¦B", "ç»´åº¦C", ...]
        custom_xticklabels = ["1978-1996", "1997-2013", "2014-2024"]  # æ ¹æ®å®é™…eraæ•°é‡è‡ªå®šä¹‰
        custom_yticklabels = [ "å¸æ³•", "å®ˆæ³•","æ‰§æ³•","ç«‹æ³•"]  # æ ¹æ®å®é™…topicæ•°é‡è‡ªå®šä¹‰

        ax = sns.heatmap(
            df_transposed,
            annot=True,
            fmt=".3f",
            cmap="Greys",  # ä½¿ç”¨é»‘ç™½ç°è‰²è°ƒï¼Œé€‚åˆé»‘ç™½æ‰“å°
            xticklabels=custom_xticklabels,
            yticklabels=custom_yticklabels,
            annot_kws={"fontsize": 20}  # è®¾ç½®çƒ­åŠ›å›¾æ•°å­—çš„å­—ä½“å¤§å°
        )
        # è®¾ç½®xè½´å’Œyè½´labelçš„å­—ä½“å¤§å°
        ax.set_xticklabels(ax.get_xticklabels(), fontsize=20)
        ax.set_yticklabels(ax.get_yticklabels(), fontsize=25)
        # plt.title(title)
        plt.xlabel("æ—¶æœŸ", fontsize=24)
        plt.ylabel("ç±»åˆ«", fontsize=24)
        plt.tight_layout()
        plt.savefig(path / "heatmap.png", dpi=300)
        plt.close()

    def run_analysis(self, era_keyword_map, use_union=False, use_general_union=False, normalize=None):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹ï¼šè®¡ç®—ç›¸ä¼¼åº¦å¹¶ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
        """
        
        print(f"\n{'='*50}")
        print(f"å¼€å§‹åˆ†æ: {era_keyword_map}")
        print(f"Unionæ¨¡å¼: {use_union}, General Unionæ¨¡å¼: {use_general_union}, å½’ä¸€åŒ–: {normalize}")
        print(f"{'='*50}")
        
        keyword_strs = []
        for era, keywords in sorted(era_keyword_map.items()):
            if isinstance(keywords, list):
                keyword_strs.append(f"{era}-[{'+'.join(keywords)}]")
            else:
                keyword_strs.append(f"{era}-{keywords}")
        
        settings = {
            "keywords": keyword_strs,
            "union": use_union,
            "general_union": use_general_union,
            "normalize": normalize
        }
        
        output_path = self._create_output_path(settings)
        print(f"è¾“å‡ºè·¯å¾„: {output_path}")
        
        df = self.calculate_similarities(era_keyword_map, use_union, use_general_union, normalize)
        
        if df.empty:
            print(f"æ— ç›¸ä¼¼åº¦æ•°æ®: {settings}")
            return
        
        print(f"\nç›¸ä¼¼åº¦çŸ©é˜µ:")
        print(df)

        title_suffix = f" (Union: {use_union}, General Union: {use_general_union}, Normalize: {normalize})"
        
        try:
            self.plot_radar(df, output_path, "Topic Similarity Radar Chart" + title_suffix)
            print(f"é›·è¾¾å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"é›·è¾¾å›¾ç”Ÿæˆå¤±è´¥: {e}")
            
        try:
            self.plot_trend(df, output_path, "Topic Similarity Trend Chart" + title_suffix)
            print(f"è¶‹åŠ¿å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"è¶‹åŠ¿å›¾ç”Ÿæˆå¤±è´¥: {e}")
            
        try:
            self.plot_heatmap(df, output_path, "Topic Similarity Heatmap" + title_suffix)
            print(f"çƒ­åŠ›å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")
            
        print(f"åˆ†æå®Œæˆ. å›¾è¡¨ä¿å­˜åˆ°: {output_path}")


def load_models():
    """Loads word vector models for each era."""
    models = {}
    model_files = {
        'era1': 'Era1_1978-1996_wordvectors.kv',
        'era2': 'Era2_1997-2013_wordvectors.kv',
        'era3': 'Era3_2014-2024_wordvectors.kv'
    }
    for era, filename in model_files.items():
        try:
            models[era] = KeyedVectors.load(str(MODELS_DIR / filename), mmap='r')
        except FileNotFoundError:
            print(f"Warning: Model for {era} not found at {MODELS_DIR / filename}")
    return models

if __name__ == '__main__':
    print("è„šæœ¬å¼€å§‹æ‰§è¡Œ...")
    
    try:
        print("åŠ è½½æ¨¡å‹ä¸­...")
        models = load_models()
        
        if not models:
            print("é”™è¯¯: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ¨¡å‹ã€‚é€€å‡ºã€‚")
            exit()
        
        print(f"æˆåŠŸåŠ è½½äº† {len(models)} ä¸ªæ¨¡å‹: {list(models.keys())}")
            
        print("åˆå§‹åŒ–åˆ†æå™¨...")
        analyzer = LegalProcessAnalyzer(models)
        
        print("æ•°æ®åŠ è½½æˆåŠŸï¼Œå¼€å§‹åˆ†æ...")

        print("\n" + "="*80)
        print("âš–ï¸ æ³•å¾‹æµç¨‹åˆ†æ: ç«‹æ³•ã€å¸æ³•ã€æ‰§æ³•ã€å®ˆæ³•")
        print("ğŸ“Š é»˜è®¤ä½¿ç”¨ General Union + Same Era å½’ä¸€åŒ–")
        print("="*80)
        
        # æµ‹è¯•ä¸åŒçš„era-keywordç»„åˆ
        

        
        # 2. æ··åˆæ¨¡å¼: era1-æ³•åˆ¶, era2-[æ³•åˆ¶+æ³•æ²»], era3-æ³•æ²»
        mixed_keywords = {
            'era1': 'æ³•åˆ¶',
            'era2': ['æ³•åˆ¶', 'æ³•æ²»'],
            'era3': 'æ³•æ²»'
        }
        print("\n--- æ··åˆæ¨¡å¼: era1-æ³•åˆ¶, era2-[æ³•åˆ¶+æ³•æ²»], era3-æ³•æ²» ---")
        analyzer.run_analysis(mixed_keywords, use_general_union=True, normalize='same_era')
        # analyzer.run_analysis(mixed_keywords, use_general_union=True, normalize="none")
        

        
        print("\nğŸ‰ æ³•å¾‹æµç¨‹åˆ†æå®Œæˆï¼")

    except Exception as e:
        import traceback
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
