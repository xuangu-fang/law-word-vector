{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import utils\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def load_dimension_words(file_path):\n",
    "    \"\"\"\n",
    "    加载维度词表文件\n",
    "    \n",
    "    Args:\n",
    "        file_path: 维度词表文件路径\n",
    "        \n",
    "    Returns:\n",
    "        dict: 维度名称到词列表的映射\n",
    "    \"\"\"\n",
    "    dimensions = {}\n",
    "    current_dimension = None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                    \n",
    "                if line.endswith(':'):\n",
    "                    current_dimension = line[:-1]\n",
    "                    dimensions[current_dimension] = []\n",
    "                elif current_dimension:\n",
    "                    words = line.split()\n",
    "                    dimensions[current_dimension].extend(words)\n",
    "                    \n",
    "        return dimensions\n",
    "    except Exception as e:\n",
    "        print(f\"加载维度词表时出错: {e}\")\n",
    "        return {}\n",
    "\n",
    "def expand_dimension_words_by_similarity(models, dimension_words, target_word=\"法治\", \n",
    "                                       similarity_threshold=0.3, max_words_per_dim=50):\n",
    "    \"\"\"\n",
    "    基于词向量相似度扩展维度词表\n",
    "    \n",
    "    Args:\n",
    "        models: 词向量模型字典\n",
    "        dimension_words: 初始维度词表\n",
    "        target_word: 目标词（法治）\n",
    "        similarity_threshold: 相似度阈值\n",
    "        max_words_per_dim: 每个维度最大词数\n",
    "        \n",
    "    Returns:\n",
    "        dict: 扩展后的维度词表\n",
    "    \"\"\"\n",
    "    expanded_words = {dim: set(words) for dim, words in dimension_words.items()}\n",
    "    \n",
    "    # 使用最新时期的模型进行扩展\n",
    "    latest_period = max(models.keys())\n",
    "    model = models[latest_period]\n",
    "    \n",
    "    print(f\"使用 {latest_period} 模型进行词表扩展\")\n",
    "    \n",
    "    # 获取与目标词相似的词\n",
    "    if target_word in model:\n",
    "        similar_words = model.most_similar(target_word, topn=1000)\n",
    "        \n",
    "        for word, similarity in similar_words:\n",
    "            if similarity < similarity_threshold:\n",
    "                break\n",
    "                \n",
    "            # 计算该词与各维度核心词的平均相似度\n",
    "            dim_similarities = {}\n",
    "            \n",
    "            for dim, core_words in dimension_words.items():\n",
    "                similarities = []\n",
    "                for core_word in core_words:\n",
    "                    if core_word in model:\n",
    "                        try:\n",
    "                            sim = model.similarity(word, core_word)\n",
    "                            similarities.append(sim)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                if similarities:\n",
    "                    dim_similarities[dim] = np.mean(similarities)\n",
    "            \n",
    "            # 将词分配给相似度最高的维度\n",
    "            if dim_similarities:\n",
    "                best_dim = max(dim_similarities, key=dim_similarities.get)\n",
    "                if (dim_similarities[best_dim] > similarity_threshold and \n",
    "                    len(expanded_words[best_dim]) < max_words_per_dim):\n",
    "                    expanded_words[best_dim].add(word)\n",
    "    \n",
    "    # 转换回列表格式\n",
    "    result = {dim: list(words) for dim, words in expanded_words.items()}\n",
    "    \n",
    "    for dim, words in result.items():\n",
    "        print(f\"{dim}: {len(words)} 个词\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def cluster_similar_words(models, similar_words_by_period, n_clusters=4, \n",
    "                         exclude_words_path=None, top_n=150):\n",
    "    \"\"\"\n",
    "    对相似词进行聚类分析，为每个时期的模型都执行聚类\n",
    "    \n",
    "    Args:\n",
    "        models: 词向量模型字典\n",
    "        similar_words_by_period: 各时期相似词字典\n",
    "        n_clusters: 聚类数量\n",
    "        exclude_words_path: 排除词文件路径\n",
    "        top_n: 每个时期取前N个词\n",
    "        \n",
    "    Returns:\n",
    "        dict: 每个时期的聚类结果 {period: (clusters, word_vectors, cluster_labels, valid_words)}\n",
    "    \"\"\"\n",
    "    # 加载排除词\n",
    "    exclude_words = set()\n",
    "    if exclude_words_path and Path(exclude_words_path).exists():\n",
    "        exclude_words = utils.load_exclude_words(exclude_words_path)\n",
    "        print(f\"已加载 {len(exclude_words)} 个排除词\")\n",
    "    \n",
    "    # 获取所有时期的词汇并集\n",
    "    all_words = set()\n",
    "    for period, word_list in similar_words_by_period.items():\n",
    "        # 过滤排除词并取前N个\n",
    "        filtered_words = [(word, sim) for word, sim in word_list \n",
    "                         if word not in exclude_words][:top_n]\n",
    "        period_words = set(word for word, _ in filtered_words)\n",
    "        all_words.update(period_words)\n",
    "    \n",
    "    print(f\"总共收集到 {len(all_words)} 个唯一词汇\")\n",
    "    \n",
    "    # 为每个时期的模型执行聚类\n",
    "    all_results = {}\n",
    "    \n",
    "    for period, model in models.items():\n",
    "        print(f\"\\n=== 对 {period} 模型进行聚类 ===\")\n",
    "        \n",
    "        # 提取在当前模型中存在的词及其向量\n",
    "        valid_words = []\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in all_words:\n",
    "            if word in model:\n",
    "                valid_words.append(word)\n",
    "                word_vectors.append(model[word])\n",
    "        \n",
    "        if not word_vectors:\n",
    "            print(f\"{period}: 没有找到有效的词向量\")\n",
    "            all_results[period] = ({}, np.array([]), [], [])\n",
    "            continue\n",
    "        \n",
    "        word_vectors = np.array(word_vectors)\n",
    "        print(f\"{period}: 成功提取 {len(valid_words)} 个词的向量，维度: {word_vectors.shape}\")\n",
    "        \n",
    "        # 执行K-means聚类\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(word_vectors)\n",
    "        \n",
    "        # 整理聚类结果\n",
    "        clusters = {}\n",
    "        for i in range(n_clusters):\n",
    "            cluster_words = [valid_words[j] for j in range(len(valid_words)) \n",
    "                            if cluster_labels[j] == i]\n",
    "            clusters[f\"聚类{i+1}\"] = cluster_words\n",
    "            print(f\"  聚类{i+1}: {len(cluster_words)} 个词\")\n",
    "            print(f\"    前10个词: {cluster_words[:10]}\")\n",
    "        \n",
    "        # 保存当前时期的结果\n",
    "        all_results[period] = (clusters, word_vectors, cluster_labels, valid_words)\n",
    "        \n",
    "        # 保存聚类结果到文件\n",
    "        save_cluster_results(clusters, f\"topic_word/cluster_results_{period}_{n_clusters}.txt\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def visualize_clusters(word_vectors, cluster_labels, valid_words, method='tsne', \n",
    "                      max_labels=50):\n",
    "    \"\"\"\n",
    "    可视化聚类结果\n",
    "    \n",
    "    Args:\n",
    "        word_vectors: 词向量矩阵\n",
    "        cluster_labels: 聚类标签\n",
    "        valid_words: 词列表\n",
    "        method: 降维方法 ('tsne' 或 'pca')\n",
    "        max_labels: 最大显示标签数\n",
    "    \"\"\"\n",
    "    if len(word_vectors) == 0:\n",
    "        print(\"没有数据可以可视化\")\n",
    "        return\n",
    "    \n",
    "    # 降维\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(word_vectors)-1))\n",
    "    else:\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    \n",
    "    reduced_vectors = reducer.fit_transform(word_vectors)\n",
    "    \n",
    "    # 绘图\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # 为每个聚类使用不同颜色\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = cluster_labels == label\n",
    "        plt.scatter(reduced_vectors[mask, 0], reduced_vectors[mask, 1], \n",
    "                   c=[colors[i]], label=f'聚类{label+1}', alpha=0.7, s=50)\n",
    "    \n",
    "    # 添加词标签（只显示部分以避免重叠）\n",
    "    if len(valid_words) <= max_labels:\n",
    "        for i, word in enumerate(valid_words):\n",
    "            plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), \n",
    "                        fontsize=8, alpha=0.8)\n",
    "    else:\n",
    "        # 随机选择一些词显示标签\n",
    "        indices = np.random.choice(len(valid_words), max_labels, replace=False)\n",
    "        for i in indices:\n",
    "            plt.annotate(valid_words[i], \n",
    "                        (reduced_vectors[i, 0], reduced_vectors[i, 1]), \n",
    "                        fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.title(f'法治相关词汇聚类可视化 ({method.upper()})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def analyze_cluster_quality(word_vectors, cluster_labels):\n",
    "    \"\"\"分析聚类质量\"\"\"\n",
    "    if len(set(cluster_labels)) > 1:\n",
    "        silhouette = silhouette_score(word_vectors, cluster_labels)\n",
    "        calinski = calinski_harabasz_score(word_vectors, cluster_labels)\n",
    "        \n",
    "        print(f\"轮廓系数 (Silhouette Score): {silhouette:.3f}\")\n",
    "        print(f\"Calinski-Harabasz指数: {calinski:.3f}\")\n",
    "        \n",
    "        return silhouette, calinski\n",
    "    else:\n",
    "        print(\"只有一个聚类，无法计算质量指标\")\n",
    "        return None, None\n",
    "\n",
    "def save_cluster_results(clusters, output_path):\n",
    "    \"\"\"保存聚类结果到文件\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# 法治相关词汇聚类结果\\n\\n\")\n",
    "        \n",
    "        for cluster_name, words in clusters.items():\n",
    "            f.write(f\"# {cluster_name} ({len(words)}个词)\\n\")\n",
    "            f.write(f\"{cluster_name}:\\n\")\n",
    "            \n",
    "            # 每行写10个词\n",
    "            for i in range(0, len(words), 10):\n",
    "                line_words = words[i:i+10]\n",
    "                f.write(\" \".join(line_words) + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"已保存聚类结果到: {output_path}\")\n",
    "\n",
    "def calculate_dimension_similarities(models, dimension_words, target_word=\"法治\"):\n",
    "    \"\"\"\n",
    "    计算目标词与各维度的相似度\n",
    "    \n",
    "    Args:\n",
    "        models: 词向量模型字典\n",
    "        dimension_words: 维度词表字典\n",
    "        target_word: 目标词\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: 各时期各维度的相似度矩阵\n",
    "    \"\"\"\n",
    "    periods = sorted(models.keys())\n",
    "    dimensions = list(dimension_words.keys())\n",
    "    \n",
    "    # 创建结果DataFrame\n",
    "    similarity_data = []\n",
    "    \n",
    "    for period in periods:\n",
    "        model = models[period]\n",
    "        if target_word not in model:\n",
    "            print(f\"警告: '{target_word}'在{period}模型中不存在\")\n",
    "            continue\n",
    "            \n",
    "        period_similarities = {\"时期\": period}\n",
    "        \n",
    "        for dim in dimensions:\n",
    "            dim_words = dimension_words[dim]\n",
    "            similarities = []\n",
    "            \n",
    "            for word in dim_words:\n",
    "                if word in model and word != target_word:\n",
    "                    try:\n",
    "                        sim = model.similarity(target_word, word)\n",
    "                        similarities.append(sim)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if similarities:\n",
    "                avg_sim = np.mean(similarities)\n",
    "                period_similarities[dim] = avg_sim\n",
    "            else:\n",
    "                period_similarities[dim] = 0\n",
    "        \n",
    "        similarity_data.append(period_similarities)\n",
    "    \n",
    "    return pd.DataFrame(similarity_data)\n",
    "\n",
    "def plot_dimension_trends(similarity_df, title=\"法治维度语义相似度变化趋势\"):\n",
    "    \"\"\"绘制维度趋势图\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    periods = similarity_df[\"时期\"]\n",
    "    dimensions = [col for col in similarity_df.columns if col != \"时期\"]\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        plt.plot(periods, similarity_df[dim], marker='o', linewidth=2, label=dim)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"时期\")\n",
    "    plt.ylabel(\"平均相似度\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_dimension_radar(similarity_df, title=\"法治维度语义结构雷达图\"):\n",
    "    \"\"\"绘制雷达图\"\"\"\n",
    "    periods = similarity_df[\"时期\"].tolist()\n",
    "    dimensions = [col for col in similarity_df.columns if col != \"时期\"]\n",
    "    N = len(dimensions)\n",
    "    \n",
    "    # 设置角度\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # 闭合雷达图\n",
    "    \n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # 为每个时期绘制一条线\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, period in enumerate(periods):\n",
    "        values = similarity_df.iloc[i][dimensions].tolist()\n",
    "        values += values[:1]  # 闭合雷达图\n",
    "        \n",
    "        # 绘制线条\n",
    "        ax.plot(angles, values, linewidth=2, label=period, color=colors[i % len(colors)])\n",
    "        # 填充区域\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
    "    \n",
    "    # 设置标签\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(dimensions)\n",
    "    \n",
    "    # 添加图例和标题\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    plt.title(title, size=15, pad=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_dimension_heatmap(similarity_df, title=\"法治维度语义相似度热力图\"):\n",
    "    \"\"\"绘制热力图\"\"\"\n",
    "    # 准备数据\n",
    "    periods = similarity_df[\"时期\"].tolist()\n",
    "    dimensions = [col for col in similarity_df.columns if col != \"时期\"]\n",
    "    \n",
    "    # 创建矩阵\n",
    "    matrix_data = similarity_df[dimensions].values\n",
    "    \n",
    "    # 绘制热力图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(matrix_data, \n",
    "                xticklabels=dimensions, \n",
    "                yticklabels=periods,\n",
    "                annot=True, \n",
    "                fmt='.3f', \n",
    "                cmap=\"YlOrRd\", \n",
    "                linewidths=0.5)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字体已存在: /home/fangshikai/.fonts/SimHei.ttf\n",
      "刷新字体缓存...\n",
      "成功设置中文字体\n",
      "使用固定时期模型目录: /home/fangshikai/law-word-vector/models/fine_tuned_vectors_flexible\n",
      "找到 3 个模型文件:\n",
      "  加载模型: Era1_1978-1996\n",
      "  成功加载 Era1_1978-1996, 词汇量: 4874\n",
      "  加载模型: Era2_1997-2013\n",
      "  成功加载 Era2_1997-2013, 词汇量: 4992\n",
      "  加载模型: Era3_2014-2024\n",
      "  成功加载 Era3_2014-2024, 词汇量: 5000\n",
      "\n",
      "成功加载了 3 个模型:\n",
      "  Era1_1978-1996: 词汇量 4874\n",
      "  Era2_1997-2013: 词汇量 4992\n",
      "  Era3_2014-2024: 词汇量 5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 设置更好的可视化风格\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "# 假设notebooks目录在项目根目录下\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "from src.utils import download_chinese_font\n",
    "import src.utils as utils\n",
    "# 下载并安装字体\n",
    "font_path = download_chinese_font()\n",
    "\n",
    "# 设置matplotlib使用下载的字体\n",
    "if font_path:\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei'] + plt.rcParams['font.sans-serif']\n",
    "    print(\"成功设置中文字体\")\n",
    "else:\n",
    "    print(\"无法设置中文字体，将使用替代方案\")\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "\n",
    "\n",
    "# 测试中文显示\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.title(\"中文测试\")\n",
    "# plt.text(0.5, 0.5, \"法治\", fontsize=20, ha='center')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# 定义项目根目录和模型目录\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# 可能的模型目录\n",
    "FINE_TUNED_MODELS_DIR = MODELS_DIR / \"fine_tuned_vectors_flexible\"\n",
    "SLIDING_WINDOW_MODELS_DIR = MODELS_DIR / \"fine_tuned_vectors_sliding_window\"\n",
    "\n",
    "# 检查哪个目录存在并包含模型\n",
    "if FINE_TUNED_MODELS_DIR.exists() and any(FINE_TUNED_MODELS_DIR.glob(\"*_wordvectors.kv\")):\n",
    "    MODELS_DIR = FINE_TUNED_MODELS_DIR\n",
    "    print(f\"使用固定时期模型目录: {MODELS_DIR}\")\n",
    "elif SLIDING_WINDOW_MODELS_DIR.exists():\n",
    "    # 查找滑动窗口模型的子目录\n",
    "    subdirs = [d for d in SLIDING_WINDOW_MODELS_DIR.iterdir() if d.is_dir()]\n",
    "    if subdirs:\n",
    "        MODELS_DIR = subdirs[0]  # 使用第一个子目录\n",
    "        print(f\"使用滑动窗口模型目录: {MODELS_DIR}\")\n",
    "    else:\n",
    "        print(f\"滑动窗口模型目录存在，但没有子目录\")\n",
    "else:\n",
    "    print(f\"未找到模型目录，使用默认路径: {MODELS_DIR}\")\n",
    "\n",
    "# 加载所有可用的模型\n",
    "def load_models():\n",
    "    \"\"\"加载目录中所有的词向量模型\"\"\"\n",
    "    models = {}\n",
    "    model_files = list(MODELS_DIR.glob(\"*_wordvectors.kv\"))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"在 {MODELS_DIR} 中没有找到模型文件\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"找到 {len(model_files)} 个模型文件:\")\n",
    "    for model_file in sorted(model_files):\n",
    "        period_name = model_file.stem.replace(\"_wordvectors\", \"\")\n",
    "        print(f\"  加载模型: {period_name}\")\n",
    "        try:\n",
    "            models[period_name] = KeyedVectors.load(str(model_file))\n",
    "            print(f\"  成功加载 {period_name}, 词汇量: {len(models[period_name].index_to_key)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  加载 {period_name} 失败: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# 加载模型\n",
    "models = load_models()\n",
    "\n",
    "# 检查模型是否成功加载\n",
    "if not models:\n",
    "    print(\"没有成功加载任何模型，请检查模型路径\")\n",
    "else:\n",
    "    print(f\"\\n成功加载了 {len(models)} 个模型:\")\n",
    "    for period_name, model in models.items():\n",
    "        print(f\"  {period_name}: 词汇量 {len(model.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 Era1_1978-1996: 150 个词\n",
      "加载 Era2_1997-2013: 150 个词\n",
      "加载 Era3_2014-2024: 150 个词\n"
     ]
    }
   ],
   "source": [
    "# 创建输出目录\n",
    "topic_word_dir = Path(\"topic_word\")\n",
    "topic_word_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "era_files = {\n",
    "    'Era1_1978-1996': 'similar_words/Era1_1978-1996_final.txt',\n",
    "    'Era2_1997-2013': 'similar_words/Era2_1997-2013_final.txt',\n",
    "    'Era3_2014-2024': 'similar_words/Era3_2014-2024_final.txt'\n",
    "}\n",
    "\n",
    "similar_words_by_period = {}\n",
    "for era, file_path in era_files.items():\n",
    "    word_list = utils.load_expert_word_list(file_path)\n",
    "    if word_list:\n",
    "        similar_words_by_period[era] = word_list\n",
    "        print(f\"加载 {era}: {len(word_list)} 个词\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 执行4聚类分析 ===\n",
      "已加载 17 个排除词\n",
      "总共收集到 284 个唯一词汇\n",
      "\n",
      "=== 对 Era1_1978-1996 模型进行聚类 ===\n",
      "Era1_1978-1996: 成功提取 181 个词的向量，维度: (181, 300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  聚类1: 54 个词\n",
      "    前10个词: ['推进改革', '思想', '德治', '严明', '法律常识', '综合治理', '严厉打击', '现代化', '至上', '公安队伍']\n",
      "  聚类2: 41 个词\n",
      "    前10个词: ['依法治国', '违法必究', '懂法', '有法', '执法人员', '严格执法', '依法', '法律制裁', '依法行事', '法律意识']\n",
      "  聚类3: 35 个词\n",
      "    前10个词: ['司法制度', '经济纠纷', '刑事', '赔偿法', '法律监督', '行政诉讼', '人民意志', '刑法', '法律', '司法机关']\n",
      "  聚类4: 51 个词\n",
      "    前10个词: ['制度', '法律法规', '市场经济', '规范化', '法规', '公平', '地方性', '民主化', '管理工作', '建立健全']\n",
      "已保存聚类结果到: topic_word/cluster_results_Era1_1978-1996_4.txt\n",
      "\n",
      "=== 对 Era2_1997-2013 模型进行聚类 ===\n",
      "Era2_1997-2013: 成功提取 249 个词的向量，维度: (249, 300)\n",
      "  聚类1: 22 个词\n",
      "    前10个词: ['人权意识', '违法必究', '尊法', '程序法', '人民意志', '司法权威', '依法行事', '司法腐败', '依宪', '司法权']\n",
      "  聚类2: 63 个词\n",
      "    前10个词: ['友爱', '思想', '实效性', '道德素质', '懂法', '德治', '失范', '化解矛盾', '法律常识', '物质文明']\n",
      "  聚类3: 79 个词\n",
      "    前10个词: ['依法治国', '司法制度', '国家治理', '刑事', '善治', '行政法', '法律监督', '文明执法', '公权', '民主化']\n",
      "  聚类4: 85 个词\n",
      "    前10个词: ['推进改革', '依法打击', '制度', '法律法规', '市场经济', '规范化', '严明', '法规', '公平', '地方性']\n",
      "已保存聚类结果到: topic_word/cluster_results_Era2_1997-2013_4.txt\n",
      "\n",
      "=== 对 Era3_2014-2024 模型进行聚类 ===\n",
      "Era3_2014-2024: 成功提取 233 个词的向量，维度: (233, 300)\n",
      "  聚类1: 43 个词\n",
      "    前10个词: ['社会主义法治理论', '依法治国', '民法典', '司法制度', '国家治理', '善治', '立法', '人民意志', '刑法', '宪法']\n",
      "  聚类2: 125 个词\n",
      "    前10个词: ['推进改革', '思想', '依法打击', '制度', '规范市场', '法律法规', '实效性', '刑事', '营商', '市场经济']\n",
      "  聚类3: 39 个词\n",
      "    前10个词: ['惩恶扬善', '违法必究', '尊法', '懂法', '文明执法', '法律常识', '严格执法', '司法权威', '公信', '法律意识']\n",
      "  聚类4: 26 个词\n",
      "    前10个词: ['道德素质', '德治', '严明', '教育引导', '职业道德', '行为习惯', '自律', '道德规范', '纪律', '思想道德']\n",
      "已保存聚类结果到: topic_word/cluster_results_Era3_2014-2024_4.txt\n",
      "\n",
      "=== 执行3聚类分析 ===\n",
      "已加载 17 个排除词\n",
      "总共收集到 284 个唯一词汇\n",
      "\n",
      "=== 对 Era1_1978-1996 模型进行聚类 ===\n",
      "Era1_1978-1996: 成功提取 181 个词的向量，维度: (181, 300)\n",
      "  聚类1: 41 个词\n",
      "    前10个词: ['司法制度', '经济纠纷', '刑事', '法律监督', '执法人员', '严格执法', '严厉打击', '公安队伍', '行政诉讼', '刑法']\n",
      "  聚类2: 38 个词\n",
      "    前10个词: ['依法治国', '违法必究', '赔偿法', '懂法', '有法', '法律常识', '人民意志', '法制宣传', '依法行事', '法律意识']\n",
      "  聚类3: 102 个词\n",
      "    前10个词: ['推进改革', '思想', '制度', '法律法规', '市场经济', '德治', '规范化', '严明', '法规', '公平']\n",
      "已保存聚类结果到: topic_word/cluster_results_Era1_1978-1996_3.txt\n",
      "\n",
      "=== 对 Era2_1997-2013 模型进行聚类 ===\n",
      "Era2_1997-2013: 成功提取 249 个词的向量，维度: (249, 300)\n",
      "  聚类1: 46 个词\n",
      "    前10个词: ['司法制度', '刑事', '行政法', '法律监督', '文明执法', '程序法', '行政诉讼', '刑法', '司法权威', '司法机关']\n",
      "  聚类2: 142 个词\n",
      "    前10个词: ['友爱', '推进改革', '思想', '依法打击', '制度', '法律法规', '实效性', '道德素质', '市场经济', '失范']\n",
      "  聚类3: 61 个词\n",
      "    前10个词: ['依法治国', '国家治理', '人权意识', '违法必究', '尊法', '懂法', '德治', '善治', '公权', '法律常识']\n",
      "已保存聚类结果到: topic_word/cluster_results_Era2_1997-2013_3.txt\n",
      "\n",
      "=== 对 Era3_2014-2024 模型进行聚类 ===\n",
      "Era3_2014-2024: 成功提取 233 个词的向量，维度: (233, 300)\n",
      "  聚类1: 47 个词\n",
      "    前10个词: ['惩恶扬善', '违法必究', '尊法', '道德素质', '懂法', '德治', '法律常识', '司法权威', '法制宣传', '公信']\n",
      "  聚类2: 136 个词\n",
      "    前10个词: ['推进改革', '思想', '依法打击', '制度', '规范市场', '法律法规', '实效性', '刑事', '营商', '市场经济']\n",
      "  聚类3: 50 个词\n",
      "    前10个词: ['社会主义法治理论', '依法治国', '民法典', '司法制度', '国家治理', '善治', '法律监督', '文明执法', '严格执法', '立法']\n",
      "已保存聚类结果到: topic_word/cluster_results_Era3_2014-2024_3.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 执行4聚类分析 ===\")\n",
    "all_results_cluster_4= cluster_similar_words(\n",
    "    models, similar_words_by_period, n_clusters=4, \n",
    "    exclude_words_path=\"exclude_words.txt\", top_n=150\n",
    ")\n",
    "\n",
    "print(\"\\n=== 执行3聚类分析 ===\")\n",
    "all_results_cluster_3= cluster_similar_words(\n",
    "    models, similar_words_by_period, n_clusters=3, \n",
    "    exclude_words_path=\"exclude_words.txt\", top_n=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. 可视化聚类结果\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m word_vectors_4,cluster_labels_4,valid_words_4\u001b[38;5;241m=\u001b[39mall_results_cluster_4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEra1_1978-1996\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== 可视化4聚类结果 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m visualize_clusters(word_vectors_4, cluster_labels_4, valid_words_4, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsne\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "    # 4. 可视化聚类结果\n",
    "word_vectors_4,cluster_labels_4,valid_words_4=all_results_cluster_4['Era1_1978-1996']\n",
    "print(\"\\n=== 可视化4聚类结果 ===\")\n",
    "visualize_clusters(word_vectors_4, cluster_labels_4, valid_words_4, method='tsne')\n",
    "    \n",
    "    # if len(word_vectors_3) > 0:\n",
    "    #     print(\"\\n=== 可视化3聚类结果 ===\")\n",
    "    #     visualize_clusters(word_vectors_3, cluster_labels_3, valid_words_3, method='tsne')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存聚类结果到: topic_word/cluster_results_3.txt\n",
      "已保存聚类结果到: topic_word/cluster_results_4.txt\n"
     ]
    }
   ],
   "source": [
    "if clusters_3:\n",
    "        save_cluster_results(clusters_3, topic_word_dir / \"cluster_results_3.txt\")\n",
    "if clusters_4:\n",
    "        save_cluster_results(clusters_4, topic_word_dir / \"cluster_results_4.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law_word_vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
